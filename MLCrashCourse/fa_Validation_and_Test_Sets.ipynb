{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM97aiZDmy7b09d17fPuiy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhazalehKeyvani/Avina/blob/main/MLCrashCourse/fa_Validation_and_Test_Sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   جدا کردن دیتاست داده های آموزشی ، اعتبار سنجی و تست\n",
        "\n",
        "*   آنالیز تفاوت داده های آموزشی و اعتبارسنجی\n",
        "*    تست مدل آموزش دیده و پدیده ی overfitting\n",
        "\n",
        "\n",
        "*   تشخیص و تعمیر مشکلات آموزشی مسئله\n",
        "\n"
      ],
      "metadata": {
        "id": "C8LE2GwOwuXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "مجموعه داده\n",
        "همانند تمرین قبلی، این تمرین از مجموعه داده مسکن کالیفرنیا برای پیش‌بینی مقدار_خانه_متوسط در سطح بلوک شهر استفاده می‌کند. مانند بسیاری از مجموعه داده های \"مشهور\"، مجموعه داده مسکن کالیفرنیا در واقع از دو مجموعه داده جداگانه تشکیل شده است که هر کدام در فایل های csv. جداگانه زندگی می کنند:\n",
        "\n",
        "مجموعه آموزشی در california_housing_train.csv است.\n",
        "مجموعه تست در california_housing_test.csv است.\n",
        "با تقسیم مجموعه آموزشی دانلود شده به دو قسمت، مجموعه اعتبارسنجی را ایجاد خواهید کرد:\n",
        "\n",
        "یک مجموعه آموزشی کوچکتر\n",
        "یک مجموعه اعتبار سنجی"
      ],
      "metadata": {
        "id": "kEDXjPjMzjnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@title Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "metadata": {
        "id": "sSQkBZbz0XnC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D8GgUovHbG0"
      },
      "source": [
        "#@title Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##بارگیری دیتاست ها از اینترنت\n",
        "کد زیر دیتاست ها را از اینترنت بارگیری میکند\n",
        "\n",
        "* `train_df`\n",
        "* `test_df`"
      ],
      "metadata": {
        "id": "sADC8f3B0q0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "uDjTOy6e0Y1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##مقیاس مقادیر برچسب ها\n"
      ],
      "metadata": {
        "id": "mrOcez1j2FHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale_factor = 1000.0\n",
        "\n",
        "train_df[\"median_house_value\"] /= scale_factor\n",
        "\n",
        "test_df[\"median_house_value\"] /= scale_factor"
      ],
      "metadata": {
        "id": "dWLubTFc2U1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wcjWcGBK3RQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the functions that build and train a model\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add one linear layer to the model to yield a simple linear regressor.\n",
        "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
        "\n",
        "\n",
        "\n",
        "  # Compile the model topography into code that TensorFlow can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model, df, feature, label, my_epochs,\n",
        "                my_batch_size=None, my_validation_split=0.1):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  history = model.fit(x=df[feature],\n",
        "                      y=df[label],\n",
        "                      batch_size=my_batch_size,\n",
        "                      epochs=my_epochs,\n",
        "                      validation_split=my_validation_split)\n",
        "\n",
        "  # Gather the model's trained weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the\n",
        "  # rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the root mean squared error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return epochs, rmse, history.history\n",
        "\n",
        "print(\"Defined the build_model and train_model functions.\")"
      ],
      "metadata": {
        "id": "hh0zry38BliJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##تعریف توابع رسم نمودار\n",
        "\n",
        "رسم نمودار ها با تابع `plot_the_loss_curve` و تفاوت loss , epochs در مموعه های آموزشی و اعتبار سنجی"
      ],
      "metadata": {
        "id": "3x39yYl8Bkww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the plotting function\n",
        "\n",
        "def plot_the_loss_curve(epochs, mae_training, mae_validation):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n",
        "  plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n",
        "  plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  top_of_y_axis = highest_loss + (delta * 0.05)\n",
        "  bottom_of_y_axis = lowest_loss - (delta * 0.05)\n",
        "\n",
        "  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the plot_the_loss_curve function.\")"
      ],
      "metadata": {
        "id": "vPsBR_uzbA1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##تمرین 1 :کسب تجربه در جداسازی با داده های اعتبار سنجی\n",
        "\n",
        "در سلول کد زیر، متغیری به نام `validation_split` را مشاهده خواهید کرد که آن را در 0.2 مقداردهی اولیه کرده ایم. متغیر `validation_split` نسبت مجموعه آموزشی اصلی را مشخص می کند که به عنوان مجموعه اعتبار سنجی عمل می کند. مجموعه آموزشی اصلی شامل 17000 نمونه است. بنابراین، یک `validation_split` از 0.2 به این معنی است که:\n",
        "\n",
        "\n",
        "*   validation set --> 17000 * 0.2 = 3400\n",
        "*   training set --> 17000 * 0.8 = 13600\n",
        "\n",
        "\n",
        "\n",
        "*   the training set\n",
        "*   the validation set\n",
        "\n",
        "اگر داده‌های مجموعه آموزشی مشابه داده‌های مجموعه اعتبارسنجی باشد، دو منحنی ضرر و مقادیر تلفات نهایی باید تقریباً یکسان باشند. با این حال، منحنی‌های loss و مقادیر loss نهایی تقریباً یکسان **نیستند**. هوم، عجیب است.\n",
        "\n",
        "با دو یا سه مقدار مختلف validation_split آزمایش کنید. آیا مقادیر مختلف validation_split مشکل را برطرف می کند؟"
      ],
      "metadata": {
        "id": "kG4DPbvxbADJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.1\n",
        "epoch = 17\n",
        "batch_size = 100\n",
        "\n",
        "# Split the original training set into a reduced training set and a\n",
        "# validation set.\n",
        "validation_split = 0.2\n",
        "\n",
        "# Identify the feature and the label.\n",
        "feature = \"median_income\"\n",
        "lable = \"median_house_value\"\n",
        "\n",
        "# Invoke the functions to build and train the model.\n",
        "\n",
        "my_model = build_model(larning_rate)\n",
        "\n",
        "epochs, rmse, history = train_model(my_model, train_df, feature, lable,\n",
        "                                    epochs, batch_size, validation_split\n",
        "                                    )\n",
        "\n",
        "plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"],\n",
        "                    history[\"val_root_mean_squared_error\"]\n",
        "                    )"
      ],
      "metadata": {
        "id": "5Zs-bkkQqNcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تمرین 2 :\n",
        "مهم نیست که چطور داده های آموزشی و اعتبار سنجی را جداسازی می کنیم\n",
        "، از نظر معنایی منحنی lossآنها متفاوت است\n",
        "\n",
        "داده ها در ست آموزشی به اندازه کافی مشابه داده های ست اعتبار سنجی نیستند\n",
        "\n",
        "وظیفه ی شما اینه که مشخص کنید چرا منحنی loss آنها مشابه نیستند\n",
        "\n",
        "\n",
        "\n",
        "*   سلول کد قبلی شامل:\n",
        "  \n",
        "\n",
        "  *   a reduced training set (the original training set - the validation set)\n",
        "  *   the validation set\n",
        "\n",
        "\n",
        "*   پانداس متد  head را برای خروجی شامل 5 ردیف اول دیتا فریم به میدهد\n",
        "\n"
      ],
      "metadata": {
        "id": "2uwqJrDa0F04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(1000)"
      ],
      "metadata": {
        "id": "EV5HhHun0E0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تمرین 3: حل مسئله\n",
        "برای رفع این مشکل، نمونه ها را shuffle\n",
        " در مجموعه ی آموزشی قبل از جداسازی یا درهمسازی می کنیم.\n",
        "\n",
        "\n",
        "\n",
        "1.   درهم سازی داده در مجموعه داده ی آموزشی قبل از صدا زدن train_model\n",
        "\n",
        "```\n",
        "shuffled_train_test = train_df.reindex(np.random.permutation(train_df.index))\n",
        "```\n",
        "\n",
        "\n",
        "2.   و به عنوان دومین آرگومان به صورت زیر shuffle_train_model را به train_model بدهید\n",
        "\n",
        "\n",
        "```\n",
        "epochs,rmse, history = train_model(my_model, shffled_train_df, my_feature,my_lable, epochs, batch_size, validation_split)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWmYL-Ar7hXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Double-click to view the complete implementation.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.08\n",
        "epochs = 70\n",
        "batch_size = 100\n",
        "\n",
        "# Split the original training set into a reduced training set and a\n",
        "# validation set.\n",
        "validation_split = 0.2\n",
        "\n",
        "\n",
        "# Identify the feature and the label.\n",
        "my_feature = \"median_income\"    # the median income on a specific city block.\n",
        "my_label = \"median_house_value\" # the median house value on a specific city block.\n",
        "# That is, you're going to create a model that predicts house value based\n",
        "# solely on the neighborhood's median income.\n",
        "\n",
        "# Shuffle the examples.\n",
        "shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
        "\n",
        "# Invoke the functions to build and train the model. Train on the shuffled\n",
        "# training set.\n",
        "my_model = build_model(learning_rate)\n",
        "epochs, rmse, history = train_model(my_model, shuffled_train_df, my_feature,\n",
        "                                    my_label, epochs, batch_size,\n",
        "                                    validation_split)\n",
        "\n",
        "plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"],\n",
        "                            history[\"val_root_mean_squared_error\"])"
      ],
      "metadata": {
        "id": "qkZiU0EQlYtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای پاسخ دادن به سوالات زیر با validation_split آزمایش کنید:\n",
        "\n",
        "*  با اختلاط مجموعه آموزشی، آیا ضرر نهایی مجموعه آموزشی به ضرر نهایی مجموعه اعتبارسنجی نزدیکتر است؟\n",
        "*   در چه محدوده ای از مقادیر validation_split، مقادیر تلفات نهایی برای مجموعه آموزشی و مجموعه اعتبارسنجی به طور معنی داری از هم جدا می شوند؟ چرا؟"
      ],
      "metadata": {
        "id": "fFS80GJbud5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title برای پاسخ به سوالات دوبار کلیک کنید\n",
        "\n",
        "# بله، پس از به هم زدن مجموعه آموزشی اصلی،\n",
        "# باخت نهایی برای مجموعه تمرینی و\n",
        "# مجموعه اعتبارسنجی بسیار نزدیکتر شده است.\n",
        "\n",
        "# اگر validation_split < 0.15،\n",
        "# مقادیر خطای نهایی برای مجموعه آموزشی و\n",
        "# مجموعه اعتبار سنجی به طور معناداری واگرا می شود. زیرا ،\n",
        "# مجموعه اعتبارسنجی دیگر شامل نمونه های کافی نیست."
      ],
      "metadata": {
        "id": "EsnK9EoWpN3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##وظیفه 4: از مجموعه داده تست برای ارزیابی عملکرد مدل خود استفاده کنید\n",
        "مجموعه تست معمولا به عنوان قاضی نهایی کیفیت یک مدل عمل می کند. مجموعه آزمون می تواند به عنوان یک قاضی بی طرف عمل کند زیرا از نمونه های آن در آموزش مدل استفاده نشده است. سلول کد زیر را برای ارزیابی مدل با مجموعه تست اجرا کنید:"
      ],
      "metadata": {
        "id": "rAcZ64ojy51A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_df[my_feature]\n",
        "y_test = test_df[my_label]\n",
        "\n",
        "results = my_model.evaluate(x_test,x_test,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "HfquefIMymsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ریشه میانگین مربعات خطای مدل را هنگام ارزیابی هر یک از سه مجموعه داده مقایسه کنید:\n",
        "\n",
        "* مجموعه آموزشی: به دنبال `root_mean_squared_error` در دوره آموزشی نهایی بگردید.\n",
        "* مجموعه اعتبارسنجی: به دنبال `val_root_mean_squared_error` در دوره آموزشی نهایی بگردید.\n",
        "* مجموعه آزمایشی: سلول کد قبلی را اجرا کنید و `root_mean_squared_error` را بررسی کنید.\n",
        "\n",
        "در حالت ایده آل، ریشه میانگین مربعات خطای هر سه مجموعه باید مشابه باشد. هستند آنها؟"
      ],
      "metadata": {
        "id": "FJ4FaMx11LI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title برای پاسخ دوبار کلیک کنید\n",
        "\n",
        "# در آزمایشات ما، بله، مقادیر rmse\n",
        "# به اندازه کافی شبیه بودند."
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y3aHNE7S1iOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}